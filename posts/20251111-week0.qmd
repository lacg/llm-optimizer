---
title: "Week 0 – Preparation"
author: "Luiz Garcia"
date: 2025-11-11
abstract: |
  Short description of what this project is about, goals, and the
  workflow you plan to follow.
doi: "10.5281/zenodo.17584973"
license: "CC-BY-NC-SA-4.0"
format: html
---

# Introduction
The LLM Optimizer will start small. The main idea is to leverage my previous knowledge using Weightless Neural Networks (WNN [@wilkie1995wisard]), Simulated Annealing (SA [@kirkpatrick1983annealing]), Tabu Search (TS [@glover1989tabu1]) and Genetic Algorithm (GA [@holland1975adaptation]) for optimization tasks and apply to a modern problem, Large Language Models (LLM [@vaswani2017attention]).

That's how a meta-heuristic prompt optimization came to be. Use NN/SA/TS/GA to discover high‑quality prompts for a target task. A tiny RAM‑net (a weightless neural network) is trained as a surrogate that quickly predicts how good a prompt will be, so the meta‑heuristic can explore millions of candidates without calling the LLM each time.


# Timeline (example)
| Week | Milestone |
|------|-----------|
| 0    | Blog launch, research plan |
| 1‑2  | Data collection set‑up |
| 3‑4  | First analyses |

# Resources
- Data repository: *link later*
- Code repo: *link later*

# References
::: {#refs}
::: 