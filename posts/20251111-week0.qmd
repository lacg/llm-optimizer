---
title: "Week 0 – Preparation"
author: "Luiz Garcia"
date: 2025-11-11
abstract: |
  Short description of what this project is about, goals, and the
  workflow you plan to follow.
doi: "10.5281/zenodo.17584973"
license: "CC-BY-NC-SA-4.0"
format: html
---

# Introduction
The LLM Optimizer will start small. The main idea is to leverage my previous knowledge using Weightless Neural Networks (WNN [@ludermir1999weightless]), Simulated Annealing (SA [@kirkpatrick1983annealing]), Tabu Search (TS [@glover1989tabu1]) and Genetic Algorithm (GA [@holland1975adaptation]) for optimization tasks and apply to a modern problem, Large Language Models (LLM [@vaswani2017attention], [@liu2022survey] and [@openai2023gpt4]).

That's how a meta-heuristic prompt optimization came to be. Use NN/SA/TS/GA to discover high‑quality prompts for a target task. A tiny RAM‑net (a weightless neural network) is trained as a surrogate that quickly predicts how good a prompt will be, so the meta‑heuristic can explore millions of candidates without calling the LLM each time.


# Timeline

| Week | Milestone |
|------|-----------|
| 0    | Blog launch and research plan |
| 1‑3  | Literature and hypothesis |
| 4‑8  | Architecture design and simulation |
| 9‑15  | Prototype and baseline |
| 16‑24  | Experiments and ablations |
| 25‑33  | Writing paper |

# Novelty
Based on the literature, these are the load-bearing facts:

1. Weightless / LUT-style layers are being revisited for energy-efficient inference and have been integrated into transformer/ViT variants (Quasi-Weightless / LUT-based layers). This shows feasibility of weightless modules inside transformer-style networks [@nag2024shrinkinggiantquasiweightless].
2. Differentiable variants of weightless networks (DWN) exist, enabling gradient-based training of lookup-table models via surrogates. That provides an option to either keep discrete WNNs and evolve parameters, or use differentiable surrogates [@bacellar2024differentiableweightlessneuralnetworks].
3. Memory-augmented transformer families and associative-memory transformer variants (for long context) have been proposed — e.g., ARMT and other memory transformer works — showing the research appetite for better memory mechanisms. This confirms the area is active, but not dominated by WNN approaches yet [@rodkin2025associativerecurrentmemorytransformer].
4. Neuroevolution / evolutionary NAS has been applied to transformer architectures for locating operations, attention variants, and hybrid operations. This means applying GA/TS/SA to discrete WNN addressing/routing is a natural and novel cross-over [@yang2023evolutionaryneuralarchitecturesearch].
5. Edge/energy papers and recent 2024–2025 WNN+Transformer/ViT works indicate momentum but show the space is still sparse — room for a focused contribution that (a) integrates WNN as KV cache replacement, (b) uses neuroevolution for discrete addressing/routing, and (c) evaluates on long-context & efficiency tradeoffs. In short: people have tried LUT layers and memory modules, but the specific combination I can propose (WNN as KV cache + GA/TS/SA optimized addressing + interpretability/energy evaluation) looks novel and publishable [@yang2023evolutionaryneuralarchitecturesearch].

**Bottom line on novelty**: prior work demonstrates feasibility (weightless modules and memory-augmented transformers) and neuroevolution for NAS; however, the exact combination — WNN modules substituting/supplementing the KV cache + evolutionary search for discrete addressing/routing + detailed M-series energy/latency evaluation and interpretability analysis — appears to be a fresh contribution space with low prior saturation and good publishability.

# References
::: {#refs}
::: 