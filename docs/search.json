[
  {
    "objectID": "llm-optimizer.html",
    "href": "llm-optimizer.html",
    "title": "LLM Optimizer Research",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "llm-optimizer.html#quarto",
    "href": "llm-optimizer.html#quarto",
    "title": "LLM Optimizer Research",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This research explores whether Weightless Neural Networks (WNNs) — specifically RAM-based neurons — can serve as a foundation for language modeling, traditionally dominated by weighted transformer architectures.\n\n\nBuilding a complete RAM-based language model pipeline:\n\nBitwiseRAMLM: 16-cluster per-bit output architecture with log-product token reconstruction\nConnectivity optimization: Genetic Algorithm + Tabu Search over neuron connectivity patterns\nRust+Metal acceleration: Full pipeline on Apple Silicon (16 CPU + 40 GPU cores)\nGating mechanisms: Content-based filtering inspired by DeepSeek’s Engram architecture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\nBest CE\nBest PPL\nBest Acc\nNotes\n\n\n\n\nTiered (50K clusters)\n~10.3\n~30,000\n~5.3%\nAsymmetric bit allocation\n\n\nBitwise (16 clusters)\n~10.1\n~24,000\n~5.8%\nPer-bit prediction + reconstruction\n\n\n\nEvaluated on WikiText-2 with GPT-2 tokenizer (50,257 vocab). For reference, GPT-2 small (124M params) achieves ~3.4 CE / ~29 PPL.\n\n\n\nFollow the research progress in the Blog.\n\n\n\nAll code is at github.com/lacg/wnn."
  },
  {
    "objectID": "index.html#current-focus",
    "href": "index.html#current-focus",
    "title": "Home",
    "section": "",
    "text": "Building a complete RAM-based language model pipeline:\n\nBitwiseRAMLM: 16-cluster per-bit output architecture with log-product token reconstruction\nConnectivity optimization: Genetic Algorithm + Tabu Search over neuron connectivity patterns\nRust+Metal acceleration: Full pipeline on Apple Silicon (16 CPU + 40 GPU cores)\nGating mechanisms: Content-based filtering inspired by DeepSeek’s Engram architecture"
  },
  {
    "objectID": "index.html#key-results",
    "href": "index.html#key-results",
    "title": "Home",
    "section": "",
    "text": "Architecture\nBest CE\nBest PPL\nBest Acc\nNotes\n\n\n\n\nTiered (50K clusters)\n~10.3\n~30,000\n~5.3%\nAsymmetric bit allocation\n\n\nBitwise (16 clusters)\n~10.1\n~24,000\n~5.8%\nPer-bit prediction + reconstruction\n\n\n\nEvaluated on WikiText-2 with GPT-2 tokenizer (50,257 vocab). For reference, GPT-2 small (124M params) achieves ~3.4 CE / ~29 PPL."
  },
  {
    "objectID": "index.html#weekly-updates",
    "href": "index.html#weekly-updates",
    "title": "Home",
    "section": "",
    "text": "Follow the research progress in the Blog."
  },
  {
    "objectID": "index.html#repository",
    "href": "index.html#repository",
    "title": "Home",
    "section": "",
    "text": "All code is at github.com/lacg/wnn."
  },
  {
    "objectID": "posts/20260202-week9.html",
    "href": "posts/20260202-week9.html",
    "title": "Week 9 – Data Model Migration and Tiered Architecture Deep Dive",
    "section": "",
    "text": "Summary\nThe most commit-heavy week (136 commits). Two parallel tracks: cleaning up the data model architecture and pushing the tiered language model to its limits with systematic grid sweeps.\n\n\nPhase Layer Removal\nThe biggest refactoring effort: removing the “Phase” layer from the data model. Previously, experiments were organized as Flow &gt; Phase &gt; Experiment. Analysis showed the Phase layer added complexity without value — experiments could be organized directly under flows.\nThis was a “Big Bang” migration affecting:\n\nDatabase schema (normalized data model)\nREST API endpoints\nDashboard frontend (all components updated)\niOS app (adapted to new data model)\nCheckpoint format\n\n\n\niOS App Improvements\nSignificant iOS work this week:\n\nXcode project setup with proper signing\nApp icon\niPad 10-column iterations view\nExpandable sections for experiment details\nGating run visualization with tier statistics\nSelf-signed certificate support for local development\n\n\n\nTiered Architecture: Grid Sweep\nRan a systematic grid sweep of tier bit configurations — 729 configurations testing different bit allocations across 3 tiers. This confirmed and refined the asymmetric insight from Week 5:\n\nBest configs consistently allocate more bits to frequent tiers\nThe optimal ratio is roughly proportional to log(data_density)\nExtreme asymmetry (e.g., 24/8/4) underperforms moderate asymmetry (20/12/8)\n\n\n\nMetal GPU Sparse Forward\nImplemented sparse forward pass on Metal GPU for the tiered architecture:\n\nNeurons with &gt;12 bits use sorted array + binary search on GPU\nNeurons with &lt;=12 bits use dense lookup on CPU\nHybrid CPU+GPU evaluation for heterogeneous architectures\n\n\n\nRAMClusterBase ABC\nRefactored the cluster layer hierarchy with a proper abstract base class (RAMClusterBase), enabling both tiered and future architectures to share the same interface for GA/TS optimization.\n\n\nGating API\nExtended the gating system with RESTful API endpoints:\n\nTrigger gating training via API call\nReal-time gating progress via WebSocket\nPer-tier gate activation statistics\n\n\n\nHybrid RAM+Transformer\nStarted exploring hybrid architectures (workstreams WS0-WS3) that combine RAM neurons for fast pattern caching with lightweight transformer layers for long-range dependencies. Early exploration, not yet yielding results.\n\n\nNext\nThe tiered architecture is hitting diminishing returns. The gating experiments suggest the problem is too many clusters (50K+). Time to try a radically different approach: bitwise output.\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/20260119-week7.html",
    "href": "posts/20260119-week7.html",
    "title": "Week 7 – Experiments Manager and Dashboard",
    "section": "",
    "text": "Summary\nWith overnight runs producing hundreds of experiment results, the need for proper experiment management became acute. This week introduced a complete tooling stack: experiments manager, REST API, and a real-time SvelteKit dashboard.\n\n\nExperiments Manager\nBuilt a system with Flow/Checkpoint abstractions:\n\nFlow: A named sequence of optimization phases (e.g., “bitwise v3”)\nCheckpoint: Serialized state at any point, enabling resume after crashes\nCLI tool: Command-line interface for creating, listing, and managing flows\n\nThis replaced ad-hoc JSON files with a structured approach to experiment tracking.\n\n\nSvelteKit Dashboard\nBuilt a web dashboard (SvelteKit, Svelte 4) for monitoring experiments:\n\nFlows CRUD: Create, view, edit, delete optimization flows\nReal-time updates: WebSocket connection for live progress\nPhase comparison tables: Side-by-side metrics across optimization phases\nCheckpoint management: Resume interrupted experiments from the UI\n\nThe dashboard connects to a FastAPI backend that wraps the experiments manager.\n\n\nGPU Sparse Evaluation\nExtended the Metal GPU accelerator to handle sparse memory groups (neurons with &gt;12 bits per neuron):\n\nTraining uses DashMap on CPU (lock-free concurrent hash map)\nEvaluation exports to sorted arrays for GPU binary search\nO(log n) lookups with coalesced memory access on Metal\n\nThis was necessary because high-bit neurons have address spaces too large for dense memory (2^20 = 1M entries per neuron).\n\n\nParallel Hybrid Genome Evaluation\nImplemented evaluate_genomes_parallel_hybrid() for maximum GA/TS throughput:\n\nMemory pool with 8 reusable instances to avoid OOM\nMultiple genomes train concurrently using the pool\nDense groups (bits &lt;= 12) evaluated on CPU, sparse groups (bits &gt; 12) on GPU\nPipelining: CPU trains batch N+1 while GPU evaluates batch N\n\nThis achieved 4–8x speedup over sequential genome evaluation.\n\n\nPhase Comparison Tables\nAdded structured reporting: after each optimization phase, the system full-evaluates the top genomes on validation data and prints a comparison table showing CE, accuracy, and improvement across all phases.\n\n\nNext\nThe dashboard is functional but basic. Next week: full CRUD, charts, and an iOS companion app.\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/20251229-week4.html",
    "href": "posts/20251229-week4.html",
    "title": "Week 4 – RAM Transformer and the Big Sprint",
    "section": "",
    "text": "Summary\nThis week saw 108 commits — a complete architecture sprint that transformed the project from a toy parity checker into a real research platform. The RAM Transformer, multiple attention mechanisms, benchmark suite, and the first language model experiments all landed.\n\n\nRAM Transformer Architecture\nBuilt a complete transformer-style architecture using RAM neurons:\n\nRAMTransformerBlock: Attention + FFN with XOR residual connections\nMultiple attention variants: SoftRAMAttention (learned), PositionOnlyAttention (computed, 100% generalization), ComputedSortingAttention, ComputedMinMaxAttention\nCross-attention: RAMCrossAttention for encoder-decoder models\nFFN variants: Including computed operations (increment, ROT13, Caesar cipher) that achieve 100% generalization\n\nThe key distinction is between learned operations (limited to trained patterns) and computed operations (100% generalization via algorithmic implementation).\n\n\nComprehensive Benchmarks\nTested across a wide range of tasks:\n\n\n\nTask\nAccuracy\nNotes\n\n\n\n\nbAbI story understanding\n100%\nSimple QA from stories\n\n\nTheorem proving\n100%\nLogical deduction\n\n\nCode completion\n100%\nPattern-based\n\n\nSorting\n100%\nComputed attention\n\n\nArithmetic\n100%\nComputed FFN\n\n\nSCAN/ListOps\nPartial\nCompositional generalization harder\n\n\nLanguage modeling\n79%\nFirst attempt, simple setup\n\n\n\n\n\nLanguage Model v2\nStarted the ram_lm_v2 benchmark — the first real attempt at WikiText-2 language modeling with RAM neurons. Key components:\n\nGPT-2 tokenizer (50,257 vocab)\nCluster-based output (neurons per token)\nPerplexity and cross-entropy scoring\nGA/TS connectivity optimization\n\nInitial results were far from transformer-level but established the evaluation framework.\n\n\nRust+Metal Accelerator\nThe Python evaluation was too slow for population-based optimization (50 genomes x full WikiText-2). Built a Rust accelerator with PyO3 bindings:\n\nrayon for CPU parallelism (16 cores)\nMetal compute shaders for GPU evaluation (40 cores)\n822x speedup over pure Python for batch evaluation\n\nThis made overnight optimization runs feasible.\n\n\nOther Notable Additions\n\nKneser-Ney smoothing, BPE tokenizer support\nContrastive learning and curriculum training\nSparse memory backend for high-bit neurons\nOverfitting detection in evaluation\n\n\n\nNext\nWith the infrastructure in place, the focus shifts to architecture search — finding the right neuron counts, bit widths, and connectivity patterns for language modeling.\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Week 10 – Bitwise Architecture and Plug-and-Play Gating\n\n\nIntroduced BitwiseRAMLM — a per-bit output language model with only 16 clusters that outperforms the 50K-cluster tiered architecture. Plus plug-and-play gating that works across both architectures.\n\n\n\n\n\n2026-02-09\n\n\nLuiz Garcia\n\n\n\n\n\n\n\nWeek 9 – Data Model Migration and Tiered Architecture Deep Dive\n\n\nMajor data model migration removing the Phase abstraction, plus deep work on tiered architecture with Metal GPU sparse forward and tier bits grid sweep.\n\n\n\n\n\n2026-02-02\n\n\nLuiz Garcia\n\n\n\n\n\n\n\nWeek 8 – Full Dashboard, iOS App, and Gating\n\n\nShipped a complete dashboard with charts and real-time updates, an iOS/iPadOS companion app, and the first gating experiments.\n\n\n\n\n\n2026-01-26\n\n\nLuiz Garcia\n\n\n\n\n\n\n\nWeek 7 – Experiments Manager and Dashboard\n\n\nBuilt an experiments management system and a SvelteKit dashboard for real-time monitoring of optimization runs.\n\n\n\n\n\n2026-01-19\n\n\nLuiz Garcia\n\n\n\n\n\n\n\nWeek 6 – Coarse-to-Fine Search and Fitness Calculators\n\n\nBuilt a structured optimization pipeline: coarse GA search narrows the space, fine TS search refines. Introduced fitness calculators for multi-objective optimization.\n\n\n\n\n\n2026-01-12\n\n\nLuiz Garcia\n\n\n\n\n\n\n\nWeek 5 – Tiered Architecture and Asymmetric Discovery\n\n\nDiscovered that asymmetric bit allocation across frequency tiers dramatically improves language model performance. Built overnight sweep infrastructure.\n\n\n\n\n\n2026-01-05\n\n\nLuiz Garcia\n\n\n\n\n\n\n\nWeek 4 – RAM Transformer and the Big Sprint\n\n\nThe most productive week yet. Built the complete RAM Transformer architecture, comprehensive benchmarks, and started the language model experiments.\n\n\n\n\n\n2025-12-29\n\n\nLuiz Garcia\n\n\n\n\n\n\n\nWeek 3 – KV Memory and Transformer Foundations\n\n\nInitial work on key-value memory mechanisms and laying the groundwork for the RAM Transformer architecture.\n\n\n\n\n\n2025-12-22\n\n\nLuiz Garcia\n\n\n\n\n\n\n\nWeek 2 – Vectorization and Architecture Cleanup\n\n\nImproving the core architecture with PyTorch vectorization and cleaning up the foundation for what will become the RAM Transformer.\n\n\n\n\n\n2025-12-15\n\n\nLuiz Garcia\n\n\n\n\n\n\n\nWeek 1 – First steps\n\n\nDescribing the baseline of the baseline architecture, the repository and the first toy experiments with results.\n\n\n\n\n\n2025-12-03\n\n\nLuiz Garcia\n\n\n\n\n\n\n\nWeek 0 – Preparation\n\n\nShort description of what this project is about, goals, and the workflow you plan to follow.\n\n\n\n\n\n2025-11-11\n\n\nLuiz Garcia\n\n\n\n\n\nNo matching items\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "posts/20251222-week3.html",
    "href": "posts/20251222-week3.html",
    "title": "Week 3 – KV Memory and Transformer Foundations",
    "section": "",
    "text": "Summary\nShort week due to holidays, but important conceptual work on how RAM neurons can implement key-value memory — a critical building block for attention mechanisms.\n\n\nKV Memory Spec\nDesigned KVSpec, a configuration class defining how RAM neurons partition their address space into key bits and value bits. The idea:\n\nk_bits determine which “head” to read/write (hard routing)\nv_bits store the actual value\nA query with all-zero value bits triggers a read; non-zero triggers a write\n\nThis is the RAM equivalent of the KV cache in weighted transformers, but with O(1) lookup instead of O(n) attention computation.\n\n\nDesign Decisions\nThe key insight this week: RAM neurons naturally implement associative memory. Given an address (key), they return the stored value in one lookup. The challenge is making this work compositionally — having multiple neurons agree on what to store and retrieve.\n\n\nNext\nWith the KV memory spec in place, the next step is building the full attention mechanism and the multi-layer RAM Transformer.\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/20260105-week5.html",
    "href": "posts/20260105-week5.html",
    "title": "Week 5 – Tiered Architecture and Asymmetric Discovery",
    "section": "",
    "text": "Summary\nThe central discovery this week: not all tokens deserve the same architecture. Frequent tokens (which have abundant training data) benefit from more bits per neuron, while rare tokens (sparse data) need fewer bits to avoid empty memory cells.\n\n\nTiered Sparse Memory\nImplemented a tiered architecture where the vocabulary is split into frequency-based tiers, each with its own neuron count and bit width:\nTier 0: 100 most frequent tokens  (46% of data)  → 15 neurons, 20 bits\nTier 1: 400 medium tokens         (13% of data)  → 10 neurons, 12 bits\nTier 2: 50K+ rare tokens          (40% of data)  →  5 neurons,  8 bits\n\n\nThe Asymmetric Insight\n\n\n\nConfiguration\nTest PPL\n\n\n\n\nAsymmetric (20/12/8 bits)\n36,853\n\n\nUniform (20/20/20 bits)\n49,675\n\n\n\nThe asymmetric config achieves 35% better perplexity. The reason is training data density per address space:\n\nTier 0 tokens have ~11,000 examples each → can fill 2^20 addresses\nTier 2 tokens have ~20 examples each → 2^20 addresses are mostly EMPTY\n\nWhen a neuron encounters an EMPTY address, it returns 0.5 (maximum entropy) — pure noise. Fewer bits = smaller address space = more cells actually trained.\n\n\nOvernight Sweep Infrastructure\nBuilt tooling for systematic architecture exploration:\n\nPer-tier metrics (accuracy broken down by tier)\nSkip-completed experiments (resume interrupted sweeps)\nValidation PPL on held-out data\nJSON output for downstream analysis\n\n\n\nPer-Cluster Optimization\nMoved from global GA/TS (same config for all clusters) to per-cluster optimization. Each of the 50K+ clusters can independently have different neuron counts and bit widths. This created a much larger search space but allowed fine-grained adaptation.\n\n\nHybrid CPU+GPU Evaluation\nExtended the Rust accelerator with a hybrid mode that splits work between CPU (rayon, 16 cores) and GPU (Metal, 40 cores) simultaneously, providing ~2x additional speedup.\n\n\nNext\nThe tier configuration space is large. Need a more structured search approach — which leads to the phased coarse-to-fine search developed next week.\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/20251111-week0.html",
    "href": "posts/20251111-week0.html",
    "title": "Week 0 – Preparation",
    "section": "",
    "text": "Introduction\nThe LLM Optimizer will start small. The main idea is to leverage my previous knowledge using Weightless Neural Networks (WNN (Teresa Bernarda Ludermir and Souto 1999)), Simulated Annealing (SA (Kirkpatrick, Gelatt, and Vecchi 1983)), Tabu Search (TS (Glover 1989)) and Genetic Algorithm (GA (Holland 1975)) for optimization tasks and apply to a modern problem, Large Language Models (LLM (Vaswani et al. 2017), (Liu et al. 2021) and (OpenAI 2023)).\nThat’s how a meta-heuristic prompt optimization came to be. Use NN/SA/TS/GA to discover high‑quality prompts for a target task. A tiny RAM‑net (a weightless neural network) is trained as a surrogate that quickly predicts how good a prompt will be, so the meta‑heuristic can explore millions of candidates without calling the LLM each time.\n\n\nTimeline\n\n\n\nWeek\nMilestone\n\n\n\n\n0\nBlog launch and research plan\n\n\n1‑3\nLiterature and hypothesis\n\n\n4‑8\nArchitecture design and simulation\n\n\n9‑15\nPrototype and baseline\n\n\n16‑24\nExperiments and ablations\n\n\n25‑33\nWriting paper\n\n\n\n\n\nNovelty\nBased on the literature, these are the load-bearing facts:\n\nWeightless / LUT-style layers are being revisited for energy-efficient inference and have been integrated into transformer/ViT variants (Quasi-Weightless / LUT-based layers). This shows feasibility of weightless modules inside transformer-style networks (Nag et al. 2024).\nDifferentiable variants of weightless networks (DWN) exist, enabling gradient-based training of lookup-table models via surrogates. That provides an option to either keep discrete WNNs and evolve parameters, or use differentiable surrogates (Bacellar et al. 2024).\nMemory-augmented transformer families and associative-memory transformer variants (for long context) have been proposed — e.g., ARMT and other memory transformer works — showing the research appetite for better memory mechanisms. This confirms the area is active, but not dominated by WNN approaches yet (Rodkin et al. 2025).\nNeuroevolution / evolutionary NAS has been applied to transformer architectures for locating operations, attention variants, and hybrid operations. This means applying GA/TS/SA to discrete WNN addressing/routing is a natural and novel cross-over (Yang et al. 2023).\nEdge/energy papers and recent 2024–2025 WNN+Transformer/ViT works indicate momentum but show the space is still sparse — room for a focused contribution that (a) integrates WNN as KV cache replacement, (b) uses neuroevolution for discrete addressing/routing, and (c) evaluates on long-context & efficiency tradeoffs. In short: people have tried LUT layers and memory modules, but the specific combination I can propose (WNN as KV cache + GA/TS/SA optimized addressing + interpretability/energy evaluation) looks novel and publishable (Yang et al. 2023).\n\nBottom line on novelty: prior work demonstrates feasibility (weightless modules and memory-augmented transformers) and neuroevolution for NAS; however, the exact combination — WNN modules substituting/supplementing the KV cache + evolutionary search for discrete addressing/routing + detailed M-series energy/latency evaluation and interpretability analysis — appears to be a fresh contribution space with low prior saturation and good publishability.\n\n\nReferences\n\n\nBacellar, Alan T. L., Zachary Susskind, Zachary Susskind, Marício Breternitz Jr, Eugene B. John, Lizy K. John, Priscila M. V. Lima, and Felipe M. G. França. 2024. “Differentiable Weightless Neural Networks.” https://doi.org/10.48550/arXiv.2410.11112v4.\n\n\nGlover, F. 1989. “Tabu Search – Part i.” ORSA Journal on Computing 1 (3): 190–206. https://doi.org/10.1287/ijoc.1.1.1.\n\n\nHolland, John H. 1975. Adaptation in Natural and Artificial Systems. University of Michigan Press. https://archive.org/details/adaptationinnatu0000holl.\n\n\nKirkpatrick, S., C. D. Gelatt, and M. P. Vecchi. 1983. “Optimization by Simulated Annealing.” Science 220 (4598): 671–80. https://doi.org/10.1126/science.220.4598.671.\n\n\nLiu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, and V. & Stoyanov. 2021. “Pre‑train, Prompt, and Predict: A Survey on Large Language Models.” arXiv Preprint. https://arxiv.org/abs/2107.13586.\n\n\nNag, Shashank, Alan T. L. Bacellar, Zachary Susskind, Anshul Jha, Logan Liberty, Aishwarya Sivakumar, Eugene B. John, et al. 2024. “Shrinking the Giant : Quasi-Weightless Transformers for Low Energy Inference.” https://doi.org/10.48550/arXiv.2411.01818.\n\n\nOpenAI. 2023. “GPT‑4 Technical Report.” arXiv Preprint. https://arxiv.org/abs/2303.08774.\n\n\nRodkin, Ivan, Yuri Kuratov, Aydar Bulatov, and Mikhail Burtsev. 2025. “Associative Recurrent Memory Transformer.” https://doi.org/10.48550/arXiv.2407.04841.\n\n\nTeresa Bernarda Ludermir, Antônio P. Braga, André de Carvalho, and Marcílio C. P. de Souto. 1999. “Weightless Neural Models: A Review of Current and Past Works.” In Neural Computing Surveys 2 - ICSI Berkeley, 41–61. Berkeley, USA. https://www.cin.ufpe.br/~tbl/artigos/vol2_2-ncs-1999.pdf.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems, 30:5998–6008. https://arxiv.org/abs/1706.03762.\n\n\nYang, Shangshang, Xiaoshan Yu, Ye Tian, Xueming Yan, Haiping Ma, and Xingyi Zhang. 2023. “Evolutionary Neural Architecture Search for Transformer in Knowledge Tracing.” https://doi.org/10.48550/arXiv.2310.01180.\n\n\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/20260126-week8.html",
    "href": "posts/20260126-week8.html",
    "title": "Week 8 – Full Dashboard, iOS App, and Gating",
    "section": "",
    "text": "Summary\nThe busiest week in terms of features shipped. The dashboard went from basic CRUD to a full monitoring platform with charts and live updates. An iOS companion app was built from scratch. And the first gating mechanism (inspired by DeepSeek’s Engram) was implemented and tested.\n\n\nDashboard v2\nMajor dashboard improvements:\n\nLive charts: Real-time CE and accuracy plots per generation/iteration\nTooltips: Hover over data points for detailed genome statistics\nWebSocket: Full duplex updates — experiments push progress to the dashboard\nFlow templates: Pre-configured optimization flows for common setups\nStop/restart: Graceful shutdown chain with checkpoint save before stopping\n\nThe data model was redesigned (v2) with the database as the source of truth, removing the intermediate “Phase” abstraction layer.\n\n\niOS/iPadOS App\nBuilt a SwiftUI companion app for monitoring experiments on the go:\n\nExperiment iteration lists with expandable detail sections\nCharts for tracking optimization progress\nFlow management (create, stop, resume)\niPad adaptive layouts with 10-column iterations view\nConnects to the same FastAPI backend as the web dashboard\n\n\n\nRAM-based Gating\nImplemented content-based gating inspired by DeepSeek’s Engram architecture:\n\nRAMGating: Dedicated RAM neurons learn which clusters should be active for each input context\nMajority voting: N neurons per gate, gate opens if majority fires TRUE\nStaged training: Train RAM first (freeze), then train gating (Stage 2)\nRust acceleration: rayon parallelization achieving 16x speedup\n\nThe gating model observes the same input bits as the main model but produces a binary mask over clusters. This is the RAM equivalent of attention-based gating.\nInitial results with gating on the tiered architecture were mixed — it didn’t consistently improve over ungated scores. The hypothesis is that 50K+ clusters (one per token) is too many for the gating model to learn meaningful patterns. This motivates the bitwise architecture developed later.\n\n\nOther Improvements\n\nNormalized harmonic fitness calculator\nAccuracy floor for GA/TS (prevents degenerate genomes)\nCheckpoint resumption improvements\nMemory leak fixes in Metal evaluators with buffer pooling\n\n\n\nNext\nThe gating underperformance with tiered architecture, combined with data density insights, points toward a fundamentally different approach — the bitwise architecture with only 16 clusters.\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/20251203-week1.html",
    "href": "posts/20251203-week1.html",
    "title": "Week 1 – First steps",
    "section": "",
    "text": "Introduction\nAfter refreshing about Weightless Neuron Networks (WNNs) and understanding more Large Language Models (LLMs), I started to develop a small RAM based architecture and a toy project to test it out and create some traction and understanding.\n\n\nArchitecture\nFirst, I started with a RAMNeuron, RAMLayer and a RAMAutomaton, but then learning more about PyTorch and how to improve a lot with vectorization, I tweaked the standard RAM neurons a little.\nSo, today, the RAMNeurons are actually implemented by a vectorization class called Memory and RAMLayer is just a wrapper class to Memory. Memory also have a MemoryVal IntEnum representing what it stores. And finally, the RAMAutomaton was replaced by a RAMTransformer, which is not yet a Transformer, as known by the LLM Perceptron architecture, but it is it’s goal to become a weightless Transformer.\nThe RAMTransformer has 3 layers based on a research I did during my undergrad years: 1. Input layer - linked directly to the input bits. 2. State layer - linked with the state layer output and with some input layer output. This allows context and make the network learn with time/context. 3. Output layer - linked with the input layer and state layer and providing the network output.\n\n\nRepository\nThe code is stored on GitHub where a more detailed explanation about the architecture is laid down.\n\n\nToy problem\nIn order to test it out the architecture, I used a simple problem, simple enough to initially not even need a state layer.\nThe problem is the parity check.\nSo, I defined three phases, where all hidden neurons were connected to the only neuron on the output layer (the parity check is a classifier of one bit, True or False for parity): 1. testing the network with only input layer (no state layer): a. x neurons with y connections, where y were smaller than the number of input bits n; but x * y &gt; n, to allow all input bits were linked to an input neuron. b. 1 neuron with n connections, where n were the input size. Output layer were connected to the only one hidden neuron. 2. testing the network with input and state layers, so we can have a window. The limitation with #1 is that the higher the parity check, the higher is the number of input neurons/bits. Having a state to give context of the previous parity checks, it allows the network to do a parity check on every m bits at a time, allowing an unlimitted size of bits to do a parity check.\n\n\nResults\n1.a. For parity checks up to 4 bits, 100% was common, if not the norm. Between 5-6, it became harder and harder, where the network started to cluster some neurons on nearby hamming distances and 90%, and 98-99% being common, but 100% rare. Above that up to 12 bits… 92-99% was the norm, depending on the parameters. As this is a toy problem used for PoC, I won’t go further on analysis of those parameters as this could be a complete paper per si and out of focus to where I want to achieve. It provided enough clarity for me to theorize 1.b. 1.b. 1 input neuron linked to all input bits and output neuron linked to the input neuron. As the parity check is a deterministic problem, if the learning algorithm for the WNN was correct, this architecture should make the network converge to the answer. I tested the parity from 1-12 with 100% and fast. Then I jumped to parity check for 20 bits and 25 bits, where is was also 100% acceptance rate, but it took 6h and 27h respectively. 2. 1.b. showed me the network and the algorithm was correct and could properly converge. And also its limitation… which brought me to the idea of an window, which the state layer would help and require an improvement on the learning algorithm. As the teaching now will need to backpropagate the training on many iterations through some paths taken on the network and its hidden layers.\nThat is where I am now.\n\n\nReferences\n\n\n\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/20260112-week6.html",
    "href": "posts/20260112-week6.html",
    "title": "Week 6 – Coarse-to-Fine Search and Fitness Calculators",
    "section": "",
    "text": "Summary\nWith the tiered architecture showing promise, the challenge became systematic search. This week introduced the coarse-to-fine phased search pipeline and fitness calculators that balance cross-entropy and accuracy.\n\n\nPhased Search Pipeline\nThe optimization now runs in phases, each optimizing one dimension while fixing others:\n\nPhase 1: GA optimizes neuron counts (bits fixed)\nPhase 2: TS refines neuron counts\nPhase 3: GA optimizes bit widths (neurons fixed)\nPhase 4: TS refines bit widths\nPhase 5: GA optimizes connectivity (architecture fixed)\nPhase 6: TS refines connectivity\n\nEach phase seeds from the previous phase’s best genome. This structured decomposition avoids the curse of simultaneously searching a massive joint space.\n\n\nFitness Calculators\nA fundamental question: should we optimize for cross-entropy (CE) or accuracy? They often disagree — a genome with the best CE might not have the best accuracy, and vice versa.\nIntroduced FitnessCalculatorType:\n\nCE: Pure cross-entropy ranking\nHARMONIC_RANK: Weighted harmonic mean of CE rank and accuracy rank\n\nThe harmonic rank balances both objectives: a genome that’s rank 1 in CE but rank 50 in accuracy scores worse than one that’s rank 3 in both. This prevents optimizing for one metric at the expense of the other.\n\n\nAdaptive Parameter Scaling\nGA and TS now support progressive accuracy thresholds — a form of curriculum learning for the optimizer itself. Early generations accept any genome; later generations require increasing minimum accuracy. This prevents the population from getting stuck on degenerate solutions.\n\n\nMetal GPU for Adaptive Evaluation\nExtended the Rust accelerator to handle heterogeneous per-cluster configurations on Metal GPU. Previously, GPU evaluation required uniform architecture; now each cluster can have different bits and neurons while still running on GPU.\n\n\nConnection-Preserving Genome\nWhen GA mutates neuron count or bit width, the genome now preserves existing connections where possible. Adding a neuron copies connections from a random existing neuron; removing drops the worst-performing one. This preserves learned connectivity patterns across mutations.\n\n\nResults\nThe phased search with YAML configs and checkpoint support enabled overnight runs that explore hundreds of configurations automatically, with crash recovery via gzip-compressed checkpoints.\n\n\nNext\nNeed tooling to manage and visualize all these experiments — which leads to the dashboard and experiments manager.\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/20251215-week2.html",
    "href": "posts/20251215-week2.html",
    "title": "Week 2 – Vectorization and Architecture Cleanup",
    "section": "",
    "text": "Summary\nA quieter week focused on engineering improvements rather than new features. The parity check experiments from Week 1 revealed that the Python-level loops in the original RAMNeuron implementation were a bottleneck. This week was about fixing that.\n\n\nVectorization\nThe main effort was converting the RAMNeuron/RAMLayer code from per-neuron Python loops to batched PyTorch tensor operations. The Memory class now handles address computation and lookup for all neurons in a single vectorized call, which eliminated the O(neurons) Python overhead.\nThis also led to cleaning up the class hierarchy:\n\nRAMNeuron was absorbed into Memory (vectorized storage)\nRAMLayer became a thin wrapper around Memory\nRAMAutomaton was renamed to prepare for the Transformer-style architecture\n\n\n\nMulti-step Training\nStarted exploring multi-step training where the network sees sequences rather than single examples. This is the precursor to the state layer backpropagation mentioned at the end of Week 1.\n\n\nNext\nThe vectorized foundation is now solid enough to build more complex architectures on top. Next: attention mechanisms and the full RAM Transformer.\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/20260209-week10.html",
    "href": "posts/20260209-week10.html",
    "title": "Week 10 – Bitwise Architecture and Plug-and-Play Gating",
    "section": "",
    "text": "Summary\nThe most significant architectural breakthrough of the project. Instead of 50K+ clusters (one per token), the BitwiseRAMLM uses just 16 clusters — one per output bit. This simple change addresses the fundamental data density problem that plagued the tiered architecture.\n\n\nBitwiseRAMLM\nThe core insight: rather than predicting “which token comes next” directly (50K-way classification), predict each bit of the token’s binary encoding independently:\n\\[\\log P(\\text{token}=t) = \\sum_{i=0}^{15} \\left[ b_i(t) \\cdot \\log P_i + (1-b_i(t)) \\cdot \\log(1-P_i) \\right]\\]\nwhere \\(b_i(t)\\) is the \\(i\\)-th bit of token \\(t\\)’s binary encoding and \\(P_i = P(\\text{bit}_i = 1 \\mid \\text{context})\\).\nWhy this works better:\n\n\n\n\n\n\n\n\n\nTiered (50K clusters)\nBitwise (16 clusters)\n\n\n\n\nClusters\n50,257\n16\n\n\nTraining examples per cluster\n~20 (rare tokens)\n~150,000 (ALL examples)\n\n\nData density\nSeverely sparse for rare tokens\nEvery neuron sees everything\n\n\nAddress space utilization\nMany EMPTY cells\nDense training\n\n\n\n\n\n4-State Memory Modes\nIntroduced QUAD memory modes for BitwiseRAMLM:\n\nTERNARY (mode 0): Original 3-state (FALSE/TRUE/EMPTY), majority vote\nQUAD_BINARY (mode 1): 4-state nudging with binary threshold (cell &gt;= 2 means true)\nQUAD_WEIGHTED (mode 2): 4-state nudging with weighted confidence\n\nThe 4-state modes handle contradictory training examples gracefully — instead of the last writer wins, cells accumulate evidence.\n\n\n7-Phase Optimization Pipeline\nBuilt a complete optimization pipeline for BitwiseRAMLM:\n\nGrid search (neurons x bits landscape scan)\nGA neurons (bits fixed from grid search)\nTS neurons refinement\nGA bits (neurons fixed)\nTS bits refinement\nGA connections (architecture fixed)\nTS connections refinement\n\n\n\nNeuron-Parallel Training + Metal CE\nThe Rust accelerator was extended for bitwise:\n\nNeuron-parallel training: Each of 16 clusters trains independently, enabling massive parallelism\nMetal GPU CE: The reconstruction matmul (16 bits x 50K tokens) runs on Metal\n23x speedup for the full train+eval pipeline\n\n\n\nFull-Rust BitwiseEvaluator\nThe BitwiseEvaluator supports heterogeneous per-cluster configs (different neurons and bits per cluster) with automatic Rust+Metal batch evaluation. 50 genomes are evaluated in parallel.\n\n\nPlug-and-Play Gating\nExtracted gating into a standalone GatingTrainer that works with both architectures:\n\nGatingMode.TOKEN_LEVEL: Universal, vocab_size gates (works with any architecture)\nGatingMode.BIT_LEVEL: Bitwise-specific, 16 gates that confidence-weight bit predictions\nGatingMode.DUAL_STAGE: Both — bit gating then token gating\n\nThe trainer is architecture-agnostic: pass cluster_order for tiered encoding, omit it for bitwise.\n\n\nRouting Experiments\nAlso explored routing-based approaches:\n\nDeterministic routing using input-observable features\nSelective expert evaluation (4x speedup by skipping irrelevant experts)\nInput feature analysis for optimal routing strategies\n\n\n\nRefactored GA/TS\nMajor cleanup of the optimization infrastructure:\n\nOptimizationConfig base class shared by GA and TS\nPluggable hooks for monitoring and intervention\nUnified optimize() loops\n\n\n\nResults\nThe bitwise architecture with QUAD_WEIGHTED memory is currently the best performer:\n\nCE ~10.1 (vs ~10.3 for best tiered)\nPPL ~24,000 (vs ~30,000 for best tiered)\nAccuracy ~5.8% (vs ~5.3% for best tiered)\n\nStill far from GPT-2’s ~3.4 CE / ~29 PPL, but the gap between tiered and bitwise demonstrates that data density is a critical bottleneck for RAM-based language models.\n\n\nNext\nContinue optimizing the bitwise architecture. The gating mechanism is now ready for systematic testing across all three modes. The DUAL_STAGE mode (bit gating + token gating) is the most promising unexplored direction.\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is an open research project investigating Weightless Neural Networks (WNNs) as an alternative to weighted neural networks for language modeling. The core question: can RAM-based lookup neurons — with O(1) inference and no floating-point weights — learn to predict language?\n\n\nThe project started as an LLM prompt optimizer (hence the repository name), using meta-heuristic algorithms (GA, TS, SA) with a WNN surrogate model. During early architecture work, the WNN language model itself became the primary research focus.\n\n\n\nRather than training weights via gradient descent, we:\n\nBinary encoding: Tokens are encoded as binary vectors. Each neuron observes a random subset of input bits (partial connectivity).\nMemory-based learning: Neurons store input-output mappings in ternary memory (TRUE/FALSE/EMPTY). New patterns are written directly — no gradients needed.\nConnectivity optimization: GA and TS search over which input bits each neuron observes. This is the generalization mechanism — neurons that see the right features generalize to unseen inputs.\nArchitecture search: Per-cluster neurons and bits are optimized, creating asymmetric architectures where frequent tokens get more resources.\n\n\n\n\nTiered RAMLM (50K+ clusters, one per vocabulary token):\n\nTokens grouped into frequency tiers with different neuron/bit allocations\nAsymmetric configs (e.g., 100 tokens @ 20 bits, 400 @ 12, rest @ 8) outperform uniform by 35%\nLimited by sparse training data for rare tokens\n\nBitwiseRAMLM (16 clusters, one per output bit):\n\nPredicts P(bit_i=1 | context) for each of 16 bits\nToken probabilities reconstructed via log-product over bits\nEvery neuron sees ALL training examples (vs ~20 per rare token in tiered)\nCurrently the best-performing architecture\n\n\n\n\nMac Studio M4 Max (2025): 16 CPU cores, 40 Metal GPU cores, 64 GB unified memory.\nCustom Rust+Metal accelerator provides ~800x speedup over pure Python, enabling population-based optimization with 50+ candidate architectures evaluated in parallel.\n\n\n\n\nDashboard: SvelteKit web app for experiment management, real-time monitoring via WebSocket\niOS app: SwiftUI companion for monitoring experiments on iPad/iPhone\nRust accelerator: PyO3-based Python extension with Metal GPU compute shaders\n\n\n\n\n\n\n\nPhase\nArchitecture search and optimization\n\n\nRepository\ngithub.com/lacg/wnn\n\n\nLicense\nCC BY-NC-SA 4.0 (prose), MIT (code)\n\n\nContact\nlacg@me.com\n\n\n\n\n\n\n@software{wnn_lm,\n  author = {Crispiniano Garcia, Luiz Alberto},\n  title  = {Weightless Neural Networks for Language Modeling},\n  year   = {2025},\n  url    = {https://github.com/lacg/wnn},\n  license = {CC BY-NC-SA 4.0}\n}"
  },
  {
    "objectID": "about.html#origin",
    "href": "about.html#origin",
    "title": "About",
    "section": "",
    "text": "The project started as an LLM prompt optimizer (hence the repository name), using meta-heuristic algorithms (GA, TS, SA) with a WNN surrogate model. During early architecture work, the WNN language model itself became the primary research focus."
  },
  {
    "objectID": "about.html#approach",
    "href": "about.html#approach",
    "title": "About",
    "section": "",
    "text": "Rather than training weights via gradient descent, we:\n\nBinary encoding: Tokens are encoded as binary vectors. Each neuron observes a random subset of input bits (partial connectivity).\nMemory-based learning: Neurons store input-output mappings in ternary memory (TRUE/FALSE/EMPTY). New patterns are written directly — no gradients needed.\nConnectivity optimization: GA and TS search over which input bits each neuron observes. This is the generalization mechanism — neurons that see the right features generalize to unseen inputs.\nArchitecture search: Per-cluster neurons and bits are optimized, creating asymmetric architectures where frequent tokens get more resources."
  },
  {
    "objectID": "about.html#key-architectures",
    "href": "about.html#key-architectures",
    "title": "About",
    "section": "",
    "text": "Tiered RAMLM (50K+ clusters, one per vocabulary token):\n\nTokens grouped into frequency tiers with different neuron/bit allocations\nAsymmetric configs (e.g., 100 tokens @ 20 bits, 400 @ 12, rest @ 8) outperform uniform by 35%\nLimited by sparse training data for rare tokens\n\nBitwiseRAMLM (16 clusters, one per output bit):\n\nPredicts P(bit_i=1 | context) for each of 16 bits\nToken probabilities reconstructed via log-product over bits\nEvery neuron sees ALL training examples (vs ~20 per rare token in tiered)\nCurrently the best-performing architecture"
  },
  {
    "objectID": "about.html#hardware",
    "href": "about.html#hardware",
    "title": "About",
    "section": "",
    "text": "Mac Studio M4 Max (2025): 16 CPU cores, 40 Metal GPU cores, 64 GB unified memory.\nCustom Rust+Metal accelerator provides ~800x speedup over pure Python, enabling population-based optimization with 50+ candidate architectures evaluated in parallel."
  },
  {
    "objectID": "about.html#tools",
    "href": "about.html#tools",
    "title": "About",
    "section": "",
    "text": "Dashboard: SvelteKit web app for experiment management, real-time monitoring via WebSocket\niOS app: SwiftUI companion for monitoring experiments on iPad/iPhone\nRust accelerator: PyO3-based Python extension with Metal GPU compute shaders"
  },
  {
    "objectID": "about.html#status",
    "href": "about.html#status",
    "title": "About",
    "section": "",
    "text": "Phase\nArchitecture search and optimization\n\n\nRepository\ngithub.com/lacg/wnn\n\n\nLicense\nCC BY-NC-SA 4.0 (prose), MIT (code)\n\n\nContact\nlacg@me.com"
  },
  {
    "objectID": "about.html#how-to-cite",
    "href": "about.html#how-to-cite",
    "title": "About",
    "section": "",
    "text": "@software{wnn_lm,\n  author = {Crispiniano Garcia, Luiz Alberto},\n  title  = {Weightless Neural Networks for Language Modeling},\n  year   = {2025},\n  url    = {https://github.com/lacg/wnn},\n  license = {CC BY-NC-SA 4.0}\n}"
  }
]