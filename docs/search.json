[
  {
    "objectID": "llm-optimizer.html",
    "href": "llm-optimizer.html",
    "title": "LLM Optimizer Research",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "llm-optimizer.html#quarto",
    "href": "llm-optimizer.html#quarto",
    "title": "LLM Optimizer Research",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the LLM Optimizer website!\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "posts/20251111-week0.html",
    "href": "posts/20251111-week0.html",
    "title": "Week‚ÄØ0 ‚Äì Preparation",
    "section": "",
    "text": "Introduction\nThe LLM Optimizer will start small. The main idea is to leverage my previous knowledge using Weightless Neural Networks (WNN (Teresa Bernarda Ludermir and Souto 1999)), Simulated Annealing (SA (Kirkpatrick, Gelatt, and Vecchi 1983)), Tabu Search (TS (Glover 1989)) and Genetic Algorithm (GA (Holland 1975)) for optimization tasks and apply to a modern problem, Large Language Models (LLM (Vaswani et al. 2017), (Liu et al. 2021) and (OpenAI 2023)).\nThat‚Äôs how a meta-heuristic prompt optimization came to be. Use NN/SA/TS/GA to discover high‚Äëquality prompts for a target task. A tiny RAM‚Äënet (a weightless neural network) is trained as a surrogate that quickly predicts how good a prompt will be, so the meta‚Äëheuristic can explore millions of candidates without calling the LLM each time.\n\n\nTimeline (example)\n\n\n\nWeek\nMilestone\n\n\n\n\n0\nBlog launch, research plan\n\n\n1‚Äë3\nLiterature & Hypothesis\n\n\n4‚Äë8\nArchitecture design & simulation\n\n\n9‚Äë15\nPrototype & baseline\n\n\n16‚Äë24\nExperiments & ablations\n\n\n25‚Äë33\nPaper\n\n\n\n\n\nNovelty\nBased on the literature, these are the load-bearing facts: :::{} 1. Weightless / LUT-style layers are being revisited for energy-efficient inference and have been integrated into transformer/ViT variants (Quasi-Weightless / LUT-based layers). This shows feasibility of weightless modules inside transformer-style networks (Nag et al. 2024). 2. Differentiable variants of weightless networks (DWN) exist, enabling gradient-based training of lookup-table models via surrogates. That provides an option to either keep discrete WNNs and evolve parameters, or use differentiable surrogates (Bacellar et al. 2024). 3. Memory-augmented transformer families and associative-memory transformer variants (for long context) have been proposed ‚Äî e.g., ARMT and other memory transformer works ‚Äî showing the research appetite for better memory mechanisms. This confirms the area is active, but not dominated by WNN approaches yet (Rodkin et al. 2025). 4. Neuroevolution / evolutionary NAS has been applied to transformer architectures for locating operations, attention variants, and hybrid operations. This means applying GA/TS/SA to discrete WNN addressing/routing is a natural and novel cross-over (Yang et al. 2023). 5. Edge/energy papers and recent 2024‚Äì2025 WNN+Transformer/ViT works indicate momentum but show the space is still sparse ‚Äî room for a focused contribution that (a) integrates WNN as KV cache replacement, (b) uses neuroevolution for discrete addressing/routing, and (c) evaluates on long-context & efficiency tradeoffs. In short: people have tried LUT layers and memory modules, but the specific combination I can propose (WNN as KV cache + GA/TS/SA optimized addressing + interpretability/energy evaluation) looks novel and publishable (Yang et al. 2023). :::\nBottom line on novelty: prior work demonstrates feasibility (weightless modules and memory-augmented transformers) and neuroevolution for NAS; however, the exact combination ‚Äî WNN modules substituting/supplementing the KV cache + evolutionary search for discrete addressing/routing + detailed M-series energy/latency evaluation and interpretability analysis ‚Äî appears to be a fresh contribution space with low prior saturation and good publishability.\n\n\nReferences\n\n\nBacellar, Alan T. L., Zachary Susskind, Zachary Susskind, Mar√≠cio Breternitz Jr, Eugene B. John, Lizy K. John, Priscila M. V. Lima, and Felipe M. G. Fran√ßa. 2024. ‚ÄúDifferentiable Weightless Neural Networks.‚Äù https://doi.org/10.48550/arXiv.2410.11112v4.\n\n\nGlover, F. 1989. ‚ÄúTabu Search ‚Äì Part i.‚Äù ORSA Journal on Computing 1 (3): 190‚Äì206. https://doi.org/10.1287/ijoc.1.1.1.\n\n\nHolland, John H. 1975. Adaptation in Natural and Artificial Systems. University of Michigan Press. https://doi.org/10.1007/978-3-642-64215-0.\n\n\nKirkpatrick, S., C. D. Gelatt, and M. P. Vecchi. 1983. ‚ÄúOptimization by Simulated Annealing.‚Äù Science 220 (4598): 671‚Äì80. https://doi.org/10.1126/science.220.4598.671.\n\n\nLiu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, and V. & Stoyanov. 2021. ‚ÄúPre‚Äëtrain, Prompt, and Predict: A Survey on Large Language Models.‚Äù arXiv Preprint. https://arxiv.org/abs/2107.13586.\n\n\nNag, Shashank, Alan T. L. Bacellar, Zachary Susskind, Anshul Jha, Logan Liberty, Aishwarya Sivakumar, Eugene B. John, et al. 2024. ‚ÄúShrinking the Giant : Quasi-Weightless Transformers for Low Energy Inference.‚Äù https://doi.org/10.48550/arXiv.2411.01818.\n\n\nOpenAI. 2023. ‚ÄúGPT‚Äë4 Technical Report.‚Äù arXiv Preprint. https://arxiv.org/abs/2303.08774.\n\n\nRodkin, Ivan, Yuri Kuratov, Aydar Bulatov, and Mikhail Burtsev. 2025. ‚ÄúAssociative Recurrent Memory Transformer.‚Äù https://doi.org/10.48550/arXiv.2407.04841.\n\n\nTeresa Bernarda Ludermir, Ant√≥nio P. Braga, Andr√© de Carvalho, and Marc√≠lio C. P. de Souto. 1999. ‚ÄúWeightless Neural Models: A Review of Current and Past Works.‚Äù In Neural Computing Surveys 2 - ICSI Berkeley, 41‚Äì61. Berkeley, USA. https://www.cin.ufpe.br/~tbl/artigos/vol2_2-ncs-1999.pdf.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. ‚ÄúAttention Is All You Need.‚Äù In Advances in Neural Information Processing Systems, 30:5998‚Äì6008. https://arxiv.org/abs/1706.03762.\n\n\nYang, Shangshang, Xiaoshan Yu, Ye Tian, Xueming Yan, Haiping Ma, and Xingyi Zhang. 2023. ‚ÄúEvolutionary Neural Architecture Search for Transformer in Knowledge Tracing.‚Äù https://doi.org/10.48550/arXiv.2310.01180.\n\n\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "LLM Optimizer Blog\nWelcome! Below you‚Äôll find all of the posts in reverse‚Äëchronological order.\nWeek 0 - Preparation\n\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "LLM Optimizer is a lightweight, open‚Äësource toolkit that helps developers and researchers tune large language models (LLMs) for specific downstream tasks.\nIt provides:\n\nüõ†Ô∏è Utilities for prompt engineering, hyper‚Äëparameter sweeps, and model‚Äëselection pipelines.\n\nüìä Visualization dashboards (built with Plotly/Dash) to inspect performance across runs.\n\nü§ù Extensible API ‚Äì you can plug in your own evaluation metrics, data loaders, or model wrappers.\n\n\nTL;DR: ‚ÄúRun fewer experiments, get better results.‚Äù\n\n\n\n\nThe rapid proliferation of LLMs has made it easy to train or fine‚Äëtune, but hard to know which fine‚Äëtuning strategy actually works for a given problem.\nLLM Optimizer fills that gap by:\n\nStandardising the experimental workflow.\n\nAutomating the tedious parts (e.g., logging, checkpoint management).\n\nSharing reproducible pipelines so the community can build on each other‚Äôs work.\n\n\n\n\n\n\n\n\n\n\n\n\nStatus\nDescription\n\n\n\n\nVersion\n0.3.0 (alpha) ‚Äì still under heavy development\n\n\nLicense\nCC BY‚Äë4.0‚ÄØNC (non‚Äëcommercial) ‚Äì see LICENSE in the repo\n\n\nContributions\nWelcome! Fork the repo, open a PR, or discuss ideas in the Issues tab.\n\n\nDocumentation\nFull docs are in the docs/ folder and on the website (this page).\n\n\nContact\nlacg@me.com\n\n\n\n\n\n\n\nIf you use LLM Optimizer in a publication, please cite it as:\n```bibtex (software?){llm_optimizer, author = {Crispiniano Garcia, Luiz Alberto}, title = {LLM Optimizer}, year = {2025}, url = {https://github.com/lacg/llm-optimizer}, license = {CC BY-NC-SA 4.0} }"
  },
  {
    "objectID": "about.html#why-was-it-created",
    "href": "about.html#why-was-it-created",
    "title": "About",
    "section": "",
    "text": "The rapid proliferation of LLMs has made it easy to train or fine‚Äëtune, but hard to know which fine‚Äëtuning strategy actually works for a given problem.\nLLM Optimizer fills that gap by:\n\nStandardising the experimental workflow.\n\nAutomating the tedious parts (e.g., logging, checkpoint management).\n\nSharing reproducible pipelines so the community can build on each other‚Äôs work."
  },
  {
    "objectID": "about.html#project-status",
    "href": "about.html#project-status",
    "title": "About",
    "section": "",
    "text": "Status\nDescription\n\n\n\n\nVersion\n0.3.0 (alpha) ‚Äì still under heavy development\n\n\nLicense\nCC BY‚Äë4.0‚ÄØNC (non‚Äëcommercial) ‚Äì see LICENSE in the repo\n\n\nContributions\nWelcome! Fork the repo, open a PR, or discuss ideas in the Issues tab.\n\n\nDocumentation\nFull docs are in the docs/ folder and on the website (this page).\n\n\nContact\nlacg@me.com"
  },
  {
    "objectID": "about.html#how-to-cite",
    "href": "about.html#how-to-cite",
    "title": "About",
    "section": "",
    "text": "If you use LLM Optimizer in a publication, please cite it as:\n```bibtex (software?){llm_optimizer, author = {Crispiniano Garcia, Luiz Alberto}, title = {LLM Optimizer}, year = {2025}, url = {https://github.com/lacg/llm-optimizer}, license = {CC BY-NC-SA 4.0} }"
  }
]