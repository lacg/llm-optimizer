[
  {
    "objectID": "llm-optimizer.html",
    "href": "llm-optimizer.html",
    "title": "LLM Optimizer Research",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "llm-optimizer.html#quarto",
    "href": "llm-optimizer.html#quarto",
    "title": "LLM Optimizer Research",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the LLM Optimizer website!\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "posts/20251111-week0.html",
    "href": "posts/20251111-week0.html",
    "title": "Week‚ÄØ0 ‚Äì Preparation",
    "section": "",
    "text": "Introduction\nThe LLM Optimizer will start small. The main idea is to leverage my previous knowledge using Weightless Neural Networks (WNN (Wilkie and Storie 1995)), Simulated Annealing (SA (Kirkpatrick, Gelatt, and Vecchi 1983)), Tabu Search (TS (Glover 1989)) and Genetic Algorithm (GA (Holland 1975)) for optimization tasks and apply to a modern problem, Large Language Models (LLM (Vaswani et al. 2017)).\nThat‚Äôs how a meta-heuristic prompt optimization came to be. Use NN/SA/TS/GA to discover high‚Äëquality prompts for a target task. A tiny RAM‚Äënet (a weightless neural network) is trained as a surrogate that quickly predicts how good a prompt will be, so the meta‚Äëheuristic can explore millions of candidates without calling the LLM each time.\n\n\nTimeline (example)\n\n\n\nWeek\nMilestone\n\n\n\n\n0\nBlog launch, research plan\n\n\n1‚Äë2\nData collection set‚Äëup\n\n\n3‚Äë4\nFirst analyses\n\n\n\n\n\nResources\n\nData repository: link later\nCode repo: link later\n\n\n\nReferences\n\n\nGlover, F. 1989. ‚ÄúTabu Search ‚Äì Part i.‚Äù ORSA Journal on Computing 1 (3): 190‚Äì206. https://doi.org/10.1287/ijoc.1.1.1.\n\n\nHolland, John H. 1975. Adaptation in Natural and Artificial Systems. University of Michigan Press. https://doi.org/10.1007/978-3-642-64215-0.\n\n\nKirkpatrick, S., C. D. Gelatt, and M. P. Vecchi. 1983. ‚ÄúOptimization by Simulated Annealing.‚Äù Science 220 (4598): 671‚Äì80. https://doi.org/10.1126/science.220.4598.671.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. ‚ÄúAttention Is All You Need.‚Äù In Advances in Neural Information Processing Systems, 30:5998‚Äì6008. https://arxiv.org/abs/1706.03762.\n\n\nWilkie, Christopher, and David Storie. 1995. ‚ÄúThe WiSARD: A Weightless Neural Network for Pattern Classification.‚Äù In 1995 International Joint Conference on Neural Networks (IJCNN), 1:124‚Äì29. https://doi.org/10.1109/IJCNN.1995.560260.\n\n\n\n\n\n\nReuseCC-BY-NC-SA-4.0"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "LLM Optimizer Blog\nWelcome! Below you‚Äôll find all of the posts in reverse‚Äëchronological order.\nWeek 0 - Preparation\n\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "LLM Optimizer is a lightweight, open‚Äësource toolkit that helps developers and researchers tune large language models (LLMs) for specific downstream tasks.\nIt provides:\n\nüõ†Ô∏è Utilities for prompt engineering, hyper‚Äëparameter sweeps, and model‚Äëselection pipelines.\n\nüìä Visualization dashboards (built with Plotly/Dash) to inspect performance across runs.\n\nü§ù Extensible API ‚Äì you can plug in your own evaluation metrics, data loaders, or model wrappers.\n\n\nTL;DR: ‚ÄúRun fewer experiments, get better results.‚Äù\n\n\n\n\nThe rapid proliferation of LLMs has made it easy to train or fine‚Äëtune, but hard to know which fine‚Äëtuning strategy actually works for a given problem.\nLLM Optimizer fills that gap by:\n\nStandardising the experimental workflow.\n\nAutomating the tedious parts (e.g., logging, checkpoint management).\n\nSharing reproducible pipelines so the community can build on each other‚Äôs work.\n\n\n\n\n\n\n\n\n\n\n\n\nStatus\nDescription\n\n\n\n\nVersion\n0.3.0 (alpha) ‚Äì still under heavy development\n\n\nLicense\nCC BY‚Äë4.0‚ÄØNC (non‚Äëcommercial) ‚Äì see LICENSE in the repo\n\n\nContributions\nWelcome! Fork the repo, open a PR, or discuss ideas in the Issues tab.\n\n\nDocumentation\nFull docs are in the docs/ folder and on the website (this page).\n\n\nContact\nlacg@me.com\n\n\n\n\n\n\n\nIf you use LLM Optimizer in a publication, please cite it as:\n```bibtex (software?){llm_optimizer, author = {Crispiniano Garcia, Luiz Alberto}, title = {LLM Optimizer}, year = {2025}, url = {https://github.com/lacg/llm-optimizer}, license = {CC BY-NC-SA 4.0} }"
  },
  {
    "objectID": "about.html#why-was-it-created",
    "href": "about.html#why-was-it-created",
    "title": "About",
    "section": "",
    "text": "The rapid proliferation of LLMs has made it easy to train or fine‚Äëtune, but hard to know which fine‚Äëtuning strategy actually works for a given problem.\nLLM Optimizer fills that gap by:\n\nStandardising the experimental workflow.\n\nAutomating the tedious parts (e.g., logging, checkpoint management).\n\nSharing reproducible pipelines so the community can build on each other‚Äôs work."
  },
  {
    "objectID": "about.html#project-status",
    "href": "about.html#project-status",
    "title": "About",
    "section": "",
    "text": "Status\nDescription\n\n\n\n\nVersion\n0.3.0 (alpha) ‚Äì still under heavy development\n\n\nLicense\nCC BY‚Äë4.0‚ÄØNC (non‚Äëcommercial) ‚Äì see LICENSE in the repo\n\n\nContributions\nWelcome! Fork the repo, open a PR, or discuss ideas in the Issues tab.\n\n\nDocumentation\nFull docs are in the docs/ folder and on the website (this page).\n\n\nContact\nlacg@me.com"
  },
  {
    "objectID": "about.html#how-to-cite",
    "href": "about.html#how-to-cite",
    "title": "About",
    "section": "",
    "text": "If you use LLM Optimizer in a publication, please cite it as:\n```bibtex (software?){llm_optimizer, author = {Crispiniano Garcia, Luiz Alberto}, title = {LLM Optimizer}, year = {2025}, url = {https://github.com/lacg/llm-optimizer}, license = {CC BY-NC-SA 4.0} }"
  }
]